{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\taylor.vanvalkenburg\\AppData\\Roaming\\nltk_dat\n",
      "[nltk_data]     a...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\taylor.vanvalkenburg\\AppData\\Roaming\\nltk_dat\n",
      "[nltk_data]     a...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Error loading word_tokenize: Package 'word_tokenize' not\n",
      "[nltk_data]     found in index\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\taylor.vanvalkenburg\\AppData\\Roaming\\nltk_dat\n",
      "[nltk_data]     a...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import gensim\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('word_tokenize')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "df1 = pd.read_csv('https://raw.githubusercontent.com/kcelotto/3252TermProject/master/cannabis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows with missing values\n",
    "df1 = df1.loc[(df1['Effects'] != 'None') | (df1['Flavor'] != 'None')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Preprocess the Effects and Flavor columns by turning them into arrarys of words\n",
    "    Not required for the code below, but I've left it in just in case we need it in the future\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert dtypes\n",
    "str_cols = ['Effects',\n",
    "            'Flavor',\n",
    "            'Description']\n",
    "\n",
    "for col in str_cols:\n",
    "    df1[col] = df1[col].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the arrays\n",
    "df1['Effects'] = df1['Effects'].apply(lambda x: x[0:].split(','))\n",
    "df1['Flavor'] = df1['Flavor'].apply(lambda x: x[0:].split(','))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>\n",
    "    The following section shows one method of preprocessing the text using scikit-learn's Count Vectorizer and TF-IDF. This is a bag of words model that will count instances of words in the text, and then compute the term frequency-inverse document frequency to assess the importance of words\n",
    "</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the cleaning function for scikit learn\n",
    "def clean_text_skl(text):\n",
    "    #define list of special characters\n",
    "    special_list = ['$',\n",
    "                '!'\n",
    "                '/',\n",
    "                '%',\n",
    "                '-',\n",
    "                \"'\",\n",
    "                \",\",\n",
    "                \"(\",\n",
    "                \")\",\n",
    "                '\"\"',\n",
    "                '‘',\n",
    "                '’',\n",
    "                '.',\n",
    "                ':',\n",
    "                ';',\n",
    "                '*',\n",
    "                '“',\n",
    "                '”']\n",
    "    \n",
    "    #make all words lowercase\n",
    "    clean_skl = text.lower()\n",
    "    \n",
    "    #remove special characters\n",
    "    [char for char in clean_skl if char not in special_list]\n",
    "\n",
    "    #lemmatize the text\n",
    "    [WordNetLemmatizer().lemmatize(w) for w in clean_skl]\n",
    "    \n",
    "    #remove stopwords\n",
    "    clean_skl = [token for token in word_tokenize(clean_skl) if not token in stopwords.words('english')]\n",
    "    clean_skl = ' '.join(clean_skl)\n",
    "    \n",
    "    return clean_skl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the description feature\n",
    "df1['Descr_clean_skl'] = df1['Description'].apply(clean_text_skl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate the vectorizer pipeline\n",
    "vectorizer = CountVectorizer()\n",
    "tfidf = TfidfTransformer()\n",
    "\n",
    "pipe = make_pipeline(vectorizer, tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform X\n",
    "X = pipe.fit_transform(df1['Descr_clean_skl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = descr_model[descr_model.wv.vocab]\n",
    "y = df1['Rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=54, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lasso\n",
    "lass = Lasso()\n",
    "\n",
    "lass.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lass = lass.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ridge\n",
    "ridge = Ridge()\n",
    "\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "y_pred_ridge = ridge.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree Regressor\n",
    "dtr = DecisionTreeRegressor()\n",
    "\n",
    "dtr.fit(X_train, y_train)\n",
    "\n",
    "y_pred_dtr = dtr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For context, the standard deviation of the target variable is: 0.4323196801579272\n",
      "The test rmse for the Lasso model is: 0.45377982086804636\n",
      "The test rmse for the Ridge model is: 0.4491493314384868\n",
      "The test rmse for the Decision Tree model is: 0.5496960788908559\n"
     ]
    }
   ],
   "source": [
    "#measure models\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "lass_rmse = MSE(y_test, y_pred_lass)**(1/2)\n",
    "ridge_rmse = MSE(y_test, y_pred_ridge)**(1/2)\n",
    "dtr_rmse = MSE(y_test, y_pred_dtr)**(1/2)\n",
    "\n",
    "print('For context, the standard deviation of the target variable is: {}'.format(df1['Rating'].std()))\n",
    "print('The test rmse for the Lasso model is: {}'.format(lass_rmse))\n",
    "print('The test rmse for the Ridge model is: {}'.format(ridge_rmse))\n",
    "print('The test rmse for the Decision Tree model is: {}'.format(dtr_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have models, we need to plot learning curves for all of them to see which is the best fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>\n",
    "    The models trained on the bag of words model created by CountVectorizer all perform poorly and underfit the data. To get past this, gensim's Word2Vec will be used to create word embeddings rather than a bag of words model. This model will preserve context by using the Word2Vec neural network to determine the probability of certain words appearing near other words\n",
    "</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the data will need to be preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the cleaning function\n",
    "def clean_text(text):\n",
    "    #define list of special characters\n",
    "    special_list = ['$',\n",
    "                '!'\n",
    "                '/',\n",
    "                '%',\n",
    "                '-',\n",
    "                \"'\",\n",
    "                \",\",\n",
    "                \"(\",\n",
    "                \")\",\n",
    "                '\"\"',\n",
    "                '‘',\n",
    "                '’',\n",
    "                '.',\n",
    "                ':',\n",
    "                ';',\n",
    "                '*',\n",
    "                '“',\n",
    "                '”']\n",
    "    \n",
    "    #make all words lowercase\n",
    "    clean = text.lower()\n",
    "    \n",
    "    #remove special characters\n",
    "    [char for char in clean if char not in special_list]\n",
    "\n",
    "    #lemmatize the text\n",
    "    [WordNetLemmatizer().lemmatize(w) for w in clean]\n",
    "    \n",
    "    #remove stopwords\n",
    "    clean_ex = [token for token in word_tokenize(clean) if not token in stopwords.words('english')]\n",
    "\n",
    "    return clean_ex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the description feature\n",
    "df1['Descr_clean'] = df1['Description'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the word2vec model\n",
    "w2v_mod = gensim.models.Word2Vec(df1['Descr_clean'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taylor.vanvalkenburg\\AppData\\Local\\Continuum\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "w2v_mx = w2v_mod[w2v_mod.wv.vocab]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
